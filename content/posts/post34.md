---
title: "AI Dictionary"
date: 2025-08-10T23:00:03+00:00
# weight: 1
# aliases: ["/first"]
tags: ["AI", "Dictionary", "LLM"]
author: "me"
# author: ["Me", "You"] # multiple authors
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
description: "AI Dictionary: An Essential Glossary for Modern Artificial Intelligence"
canonicalURL: "https://canonical.url/to/page"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: true
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "<image path/url>" # image path/url
    alt: "<alt text>" # alt text
    caption: "<text>" # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: true # only hide on current single page
editPost:
    URL: "https://github.com/<path_to_repo>/content"
    Text: "Suggest Changes" # edit text
    appendFilePath: true # to append file path to Edit link
---

# **AI Dictionary: An Essential Glossary for Modern Artificial Intelligence**

Artificial Intelligence evolves quickly, and keeping up with terminology is essential for anyone working in technology, business, or research. This dictionary summarises the key terms used across machine learning, deep learning, natural language processing, generative AI, and applied systems.

---

## **A**

### **AGI (Artificial General Intelligence)**

A theoretical form of AI capable of understanding, learning, and applying intelligence across any task at human level or beyond.

### **AI Alignment**

Research aimed at ensuring AI systems behave safely and in accordance with human values.

### **Algorithm**

A set of rules or steps used by a computer to solve a problem or perform a task.

### **API (Application Programming Interface)**

A structured interface allowing systems or software to communicate or integrate with AI models.

---

## **B**

### **Bias (Model Bias)**

Systematic errors in AI predictions caused by unbalanced or unrepresentative training data.

### **Batch Processing**

Running a group of inputs through a model at once, often for efficiency.

---

## **C**

### **Chatbot**

An AI system designed to simulate conversation via text or voice.

### **Computer Vision**

AI methods that enable machines to interpret and understand images or video.

### **Corpus**

A structured collection of text used to train or fine-tune language models.

---

## **D**

### **Data Augmentation**

Techniques used to expand training datasets (e.g., image rotation, noise injection).

### **Dataset**

A collection of structured or unstructured data used to train AI systems.

### **Deep Learning**

A subset of machine learning using multi-layer neural networks.

### **Diffusion Model**

A generative model that creates images or audio by reversing a noise-adding process.

---

## **E**

### **Embedding**

Numeric vector representation of text, images, or audio that captures semantic meaning.

### **Epoch**

One full pass of a training dataset through a model during training.

---

## **F**

### **Fine-Tuning**

Training a pre-existing model further on specialised data to improve performance.

### **Foundation Model**

A large-scale model trained on broad data, adaptable to many tasks (e.g., GPT, Claude).

---

## **G**

### **GAN (Generative Adversarial Network)**

Two-network architecture for generating synthetic data via generator–discriminator competition.

### **Generative AI**

AI systems capable of producing new content: text, images, code, audio, or video.

---

## **H**

### **Hallucination**

When an AI model outputs fabricated or incorrect information while appearing confident.

### **Hyperparameters**

Configurable model training settings (learning rate, batch size, number of layers).

---

## **I**

### **Inference**

Running a trained model to produce predictions or outputs.

### **LLM Inference Optimisation**

Techniques to reduce cost and latency during model execution (quantisation, caching).

---

## **J**

### **Joint Embedding Space**

A unified vector space in multimodal models where text, image, or audio embeddings coexist.

---

## **K**

### **Knowledge Distillation**

Process of transferring knowledge from a large “teacher” model to a smaller “student” model.

---

## **L**

### **LLM (Large Language Model)**

Neural networks trained on massive text datasets enabling advanced reasoning and generation.

### **Latent Space**

Hidden numerical representation of data used by generative models.

---

## **M**

### **ML (Machine Learning)**

Field enabling systems to learn patterns from data rather than explicit programming.

### **Model Weights**

Numerical parameters learned during training.

---

## **N**

### **Neural Network**

Model inspired by brain structure, consisting of interconnected nodes (neurons).

### **NLP (Natural Language Processing)**

Computational techniques enabling machines to process and generate human language.

---

## **O**

### **Overfitting**

When a model learns training data too precisely, reducing real-world performance.

### **Open-Weight Model**

AI model with publicly available weights that can be hosted or modified.

---

## **P**

### **Parameter**

A learned numerical value inside a model influencing how it makes decisions.

### **Prompt**

Text instructions provided to an LLM to control behaviour.

### **Prompt Engineering**

Crafting prompts to optimise model outputs.

---

## **Q**

### **Quantisation**

Reducing numerical precision of model weights to improve speed and reduce memory use.

---

## **R**

### **Reinforcement Learning (RL)**

Training where an agent learns through rewards and penalties.

### **RLHF (Reinforcement Learning from Human Feedback)**

Method where humans rate outputs to guide model behaviour.

### **Retrieval-Augmented Generation (RAG)**

Combining LLMs with real-time search or databases to improve accuracy.

---

## **S**

### **Self-Supervised Learning**

Training using automatically generated labels from data itself.

### **Supervised Learning**

Training with labelled data.

### **Synthetic Data**

Artificially generated data used for training or testing.

---

## **T**

### **Token**

The smallest unit of text processed by an LLM.

### **Transformer**

Neural architecture underlying modern LLMs, using attention mechanisms.

### **Training**

Process of adjusting model weights based on data.

---

## **U**

### **Unsupervised Learning**

Training models without labelled data to discover patterns or clusters.

---

## **V**

### **Vector Database**

Database optimised for storing and searching embeddings via similarity queries.

### **Vision-Language Model (VLM)**

Model combining text and image understanding.

---

## **W**

### **Weight Decay**

Regularisation technique preventing overfitting during training.

---

## **X**

### **XAI (Explainable AI)**

Methods that provide insight into how AI models reach decisions.

---

## **Y**

### **YOLO (You Only Look Once)**

Popular real-time object detection model.

---

## **Z**

### **Zero-Shot Learning**

Model capability to perform tasks without specific training examples.

### **Zero-Shot Prompting**

Prompting an LLM with no examples.


