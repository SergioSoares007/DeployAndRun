<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Fine-Tuning ChatGPT | DeployAndRun</title><meta name=keywords content="first"><meta name=description content="Fine-Tuning a Closed Source LLM like ChatGPT"><meta name=author content="Me"><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.36819bea596090d8b48cf10d9831382996197aa7e4fc86f792f7c08c9ca4d23b.css integrity="sha256-NoGb6llgkNi0jPENmDE4KZYZeqfk/Ib3kvfAjJyk0js=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/post9-copy/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/post9-copy/"><meta property="og:site_name" content="DeployAndRun"><meta property="og:title" content="Fine-Tuning ChatGPT"><meta property="og:description" content="Fine-Tuning a Closed Source LLM like ChatGPT"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-09-15T11:30:03+00:00"><meta property="article:modified_time" content="2020-09-15T11:30:03+00:00"><meta property="article:tag" content="First"><meta property="og:image" content="http://localhost:1313/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Cimage%20path/url%3E"><meta name=twitter:title content="Fine-Tuning ChatGPT"><meta name=twitter:description content="Fine-Tuning a Closed Source LLM like ChatGPT"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Fine-Tuning ChatGPT","item":"http://localhost:1313/posts/post9-copy/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Fine-Tuning ChatGPT","name":"Fine-Tuning ChatGPT","description":"Fine-Tuning a Closed Source LLM like ChatGPT","keywords":["first"],"articleBody":"Fine-Tuning a Closed Source LLM like ChatGPT: A Step-by-Step Guide In the realm of machine learning, fine-tuning a language model can significantly enhance its ability to perform specific tasks or understand particular contexts. In this blog post, we’ll explore how to fine-tune a closed-source language model like OpenAI’s ChatGPT. While direct access to the model’s parameters isn’t available as it might be with open-source models, fine-tuning it using your own dataset is still achievable.\nWe’ll walk through the steps necessary to prepare your data, upload it to the OpenAI API, and use tools like Weights \u0026 Biases (wandb.ai) for tracking your training process.\nTable of Contents Understanding Fine-Tuning Preparing Your Dataset Formatting Your Dataset in JSONL Uploading Your Dataset to OpenAI Fine-Tuning the Model Using Weights \u0026 Biases for Monitoring Conclusion Understanding Fine-Tuning Fine-tuning a language model involves taking a pre-trained model (in this case, ChatGPT) and training it further on a specific dataset. This allows the model to learn patterns and nuances that are unique to the data it will handle in a real-world application.\nImportant Considerations Non-Disclosure: Since we’re working with a closed-source model, you won’t have access to modify the model’s internal parameters directly. Instead, you provide it with additional training data. Use Cases: Fine-tuning can improve performance on niche domains like customer support, technical queries, or creative writing. Preparing Your Dataset Before we dive into formatting your data, ensure that you have a clear dataset that represents the type of interactions or content you want the model to learn from. For instance, if you want it to excel in customer service responses, your dataset should include typical user queries and the corresponding responses you would provide.\nFormatting Your Dataset in JSONL OpenAI’s fine-tuning endpoint requires the dataset to be in a specific format known as JSONL (JSON Lines). Each line in a JSONL file represents a training example in a JSON object. The basic structure for conversation data would look like this:\n{\"prompt\": \"User: How do I reset my password?\\nAI:\", \"completion\": \" Here's how you can reset your password...\"}\r{\"prompt\": \"User: I need help with my order.\\nAI:\", \"completion\": \"Sure, can you provide your order number?\"} Steps for Creating Your JSONL File Create a New Text File: Open your favorite text editor or IDE and create a file named training_data.jsonl. Add Your Data: Format your conversation pairs as shown in the examples above. Ensure that each example is on a new line. Save the File: Save your file in a location where you can easily access it. Uploading Your Dataset to OpenAI Once your JSONL file is prepared, the next step is to upload this dataset to OpenAI using the API.\nSteps to Upload API Key: Make sure you have your OpenAI API key. If you don’t, you’ll need to sign up and generate one from the OpenAI platform.\nInstall OpenAI Python Client:\npip install openai Upload the File: Use the following Python script to upload your dataset:\nimport openai openai.api_key = 'your-api-key' # Upload the training file response = openai.File.create( file=open(\"training_data.jsonl\", \"rb\"), purpose='fine-tune' ) print(\"File ID:\", response['id']) Fine-Tuning the Model After uploading your dataset, you can now initiate the fine-tuning process.\nSteps for Fine-Tuning Start Fine-Tuning: You can initiate fine-tuning using the following command, replacing file-id with your uploaded file’s ID.\nresponse = openai.FineTune.create(training_file=\"file-id\") print(\"Fine-tune ID:\", response['id']) Monitor the Process: While fine-tuning can take some time depending on the size of your dataset, you can monitor the training progress.\nUsing Weights \u0026 Biases for Monitoring Integrating Weights \u0026 Biases during your fine-tuning session can be highly beneficial for tracking your model’s performance metrics.\nSteps to Use Weights \u0026 Biases Install W\u0026B:\npip install wandb Initialize W\u0026B: Before running your training script, initialize W\u0026B:\nimport wandb wandb.init(project=\"fine-tuning-chatgpt\") Log Metrics: During your training loop or in callbacks, log important metrics like training loss and accuracy:\nwandb.log({\"loss\": training_loss, \"accuracy\": training_accuracy}) Now you can visualize your training progress in the W\u0026B dashboard.\nConclusion Fine-tuning a closed-source model like ChatGPT can enhance its ability to cater to specific queries, making it more valuable for your use case. By following the steps outlined in this blog post—from preparing your dataset in JSONL format to uploading it and monitoring with W\u0026B—you can leverage the full potential of ChatGPT for your needs.\nAs you embark on your fine-tuning journey, remember that iteration is key. Experiment with your dataset and monitor performance continuously to make informed adjustments and improvements. Good luck with your fine-tuning endeavors!\n","wordCount":"740","inLanguage":"en","image":"http://localhost:1313/%3Cimage%20path/url%3E","datePublished":"2020-09-15T11:30:03Z","dateModified":"2020-09-15T11:30:03Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/post9-copy/"},"publisher":{"@type":"Organization","name":"DeployAndRun","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Fine-Tuning ChatGPT</h1><div class=post-description>Fine-Tuning a Closed Source LLM like ChatGPT</div><div class=post-meta><span title='2020-09-15 11:30:03 +0000 +0000'>September 15, 2020</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;740 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/post9%20copy.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#understanding-fine-tuning>Understanding Fine-Tuning</a><ul><li><a href=#important-considerations>Important Considerations</a></li></ul></li><li><a href=#preparing-your-dataset>Preparing Your Dataset</a></li><li><a href=#formatting-your-dataset-in-jsonl>Formatting Your Dataset in JSONL</a><ul><li><a href=#steps-for-creating-your-jsonl-file>Steps for Creating Your JSONL File</a></li></ul></li><li><a href=#uploading-your-dataset-to-openai>Uploading Your Dataset to OpenAI</a><ul><li><a href=#steps-to-upload>Steps to Upload</a></li></ul></li><li><a href=#fine-tuning-the-model>Fine-Tuning the Model</a><ul><li><a href=#steps-for-fine-tuning>Steps for Fine-Tuning</a></li></ul></li><li><a href=#using-weights--biases-for-monitoring>Using Weights & Biases for Monitoring</a><ul><li><a href=#steps-to-use-weights--biases>Steps to Use Weights & Biases</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><h1 id=fine-tuning-a-closed-source-llm-like-chatgpt-a-step-by-step-guide>Fine-Tuning a Closed Source LLM like ChatGPT: A Step-by-Step Guide<a hidden class=anchor aria-hidden=true href=#fine-tuning-a-closed-source-llm-like-chatgpt-a-step-by-step-guide>#</a></h1><p>In the realm of machine learning, fine-tuning a language model can significantly enhance its ability to perform specific tasks or understand particular contexts. In this blog post, we&rsquo;ll explore how to fine-tune a closed-source language model like OpenAI&rsquo;s ChatGPT. While direct access to the model&rsquo;s parameters isn&rsquo;t available as it might be with open-source models, fine-tuning it using your own dataset is still achievable.</p><p>We&rsquo;ll walk through the steps necessary to prepare your data, upload it to the OpenAI API, and use tools like Weights & Biases (wandb.ai) for tracking your training process.</p><h2 id=table-of-contents>Table of Contents<a hidden class=anchor aria-hidden=true href=#table-of-contents>#</a></h2><ol><li><a href=#understanding-fine-tuning>Understanding Fine-Tuning</a></li><li><a href=#preparing-your-dataset>Preparing Your Dataset</a></li><li><a href=#formatting-your-dataset-in-jsonl>Formatting Your Dataset in JSONL</a></li><li><a href=#uploading-your-dataset-to-openai>Uploading Your Dataset to OpenAI</a></li><li><a href=#fine-tuning-the-model>Fine-Tuning the Model</a></li><li><a href=#using-weights--biases-for-monitoring>Using Weights & Biases for Monitoring</a></li><li><a href=#conclusion>Conclusion</a></li></ol><h2 id=understanding-fine-tuning>Understanding Fine-Tuning<a hidden class=anchor aria-hidden=true href=#understanding-fine-tuning>#</a></h2><p>Fine-tuning a language model involves taking a pre-trained model (in this case, ChatGPT) and training it further on a specific dataset. This allows the model to learn patterns and nuances that are unique to the data it will handle in a real-world application.</p><h3 id=important-considerations>Important Considerations<a hidden class=anchor aria-hidden=true href=#important-considerations>#</a></h3><ul><li><strong>Non-Disclosure:</strong> Since we&rsquo;re working with a closed-source model, you won’t have access to modify the model’s internal parameters directly. Instead, you provide it with additional training data.</li><li><strong>Use Cases:</strong> Fine-tuning can improve performance on niche domains like customer support, technical queries, or creative writing.</li></ul><h2 id=preparing-your-dataset>Preparing Your Dataset<a hidden class=anchor aria-hidden=true href=#preparing-your-dataset>#</a></h2><p>Before we dive into formatting your data, ensure that you have a clear dataset that represents the type of interactions or content you want the model to learn from. For instance, if you want it to excel in customer service responses, your dataset should include typical user queries and the corresponding responses you would provide.</p><h2 id=formatting-your-dataset-in-jsonl>Formatting Your Dataset in JSONL<a hidden class=anchor aria-hidden=true href=#formatting-your-dataset-in-jsonl>#</a></h2><p>OpenAI&rsquo;s fine-tuning endpoint requires the dataset to be in a specific format known as JSONL (JSON Lines). Each line in a JSONL file represents a training example in a JSON object. The basic structure for conversation data would look like this:</p><pre tabindex=0><code class=language-jsonl data-lang=jsonl>{&#34;prompt&#34;: &#34;User: How do I reset my password?\nAI:&#34;, &#34;completion&#34;: &#34; Here&#39;s how you can reset your password...&#34;}
{&#34;prompt&#34;: &#34;User: I need help with my order.\nAI:&#34;, &#34;completion&#34;: &#34;Sure, can you provide your order number?&#34;}
</code></pre><h3 id=steps-for-creating-your-jsonl-file>Steps for Creating Your JSONL File<a hidden class=anchor aria-hidden=true href=#steps-for-creating-your-jsonl-file>#</a></h3><ol><li><strong>Create a New Text File:</strong> Open your favorite text editor or IDE and create a file named <code>training_data.jsonl</code>.</li><li><strong>Add Your Data:</strong> Format your conversation pairs as shown in the examples above. Ensure that each example is on a new line.</li><li><strong>Save the File:</strong> Save your file in a location where you can easily access it.</li></ol><h2 id=uploading-your-dataset-to-openai>Uploading Your Dataset to OpenAI<a hidden class=anchor aria-hidden=true href=#uploading-your-dataset-to-openai>#</a></h2><p>Once your JSONL file is prepared, the next step is to upload this dataset to OpenAI using the API.</p><h3 id=steps-to-upload>Steps to Upload<a hidden class=anchor aria-hidden=true href=#steps-to-upload>#</a></h3><ol><li><p><strong>API Key:</strong> Make sure you have your OpenAI API key. If you don’t, you’ll need to sign up and generate one from the OpenAI platform.</p></li><li><p><strong>Install OpenAI Python Client:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install openai
</span></span></code></pre></div></li><li><p><strong>Upload the File:</strong>
Use the following Python script to upload your dataset:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>openai</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>openai</span><span class=o>.</span><span class=n>api_key</span> <span class=o>=</span> <span class=s1>&#39;your-api-key&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Upload the training file</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>openai</span><span class=o>.</span><span class=n>File</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>file</span><span class=o>=</span><span class=nb>open</span><span class=p>(</span><span class=s2>&#34;training_data.jsonl&#34;</span><span class=p>,</span> <span class=s2>&#34;rb&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>purpose</span><span class=o>=</span><span class=s1>&#39;fine-tune&#39;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;File ID:&#34;</span><span class=p>,</span> <span class=n>response</span><span class=p>[</span><span class=s1>&#39;id&#39;</span><span class=p>])</span>
</span></span></code></pre></div></li></ol><h2 id=fine-tuning-the-model>Fine-Tuning the Model<a hidden class=anchor aria-hidden=true href=#fine-tuning-the-model>#</a></h2><p>After uploading your dataset, you can now initiate the fine-tuning process.</p><h3 id=steps-for-fine-tuning>Steps for Fine-Tuning<a hidden class=anchor aria-hidden=true href=#steps-for-fine-tuning>#</a></h3><ol><li><p><strong>Start Fine-Tuning:</strong>
You can initiate fine-tuning using the following command, replacing <code>file-id</code> with your uploaded file&rsquo;s ID.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>openai</span><span class=o>.</span><span class=n>FineTune</span><span class=o>.</span><span class=n>create</span><span class=p>(</span><span class=n>training_file</span><span class=o>=</span><span class=s2>&#34;file-id&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Fine-tune ID:&#34;</span><span class=p>,</span> <span class=n>response</span><span class=p>[</span><span class=s1>&#39;id&#39;</span><span class=p>])</span>
</span></span></code></pre></div></li><li><p><strong>Monitor the Process:</strong>
While fine-tuning can take some time depending on the size of your dataset, you can monitor the training progress.</p></li></ol><h2 id=using-weights--biases-for-monitoring>Using Weights & Biases for Monitoring<a hidden class=anchor aria-hidden=true href=#using-weights--biases-for-monitoring>#</a></h2><p>Integrating <strong>Weights & Biases</strong> during your fine-tuning session can be highly beneficial for tracking your model&rsquo;s performance metrics.</p><h3 id=steps-to-use-weights--biases>Steps to Use Weights & Biases<a hidden class=anchor aria-hidden=true href=#steps-to-use-weights--biases>#</a></h3><ol><li><p><strong>Install W&amp;B:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install wandb
</span></span></code></pre></div></li><li><p><strong>Initialize W&amp;B:</strong>
Before running your training script, initialize W&amp;B:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>wandb</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>wandb</span><span class=o>.</span><span class=n>init</span><span class=p>(</span><span class=n>project</span><span class=o>=</span><span class=s2>&#34;fine-tuning-chatgpt&#34;</span><span class=p>)</span>
</span></span></code></pre></div></li><li><p><strong>Log Metrics:</strong>
During your training loop or in callbacks, log important metrics like training loss and accuracy:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span><span class=s2>&#34;loss&#34;</span><span class=p>:</span> <span class=n>training_loss</span><span class=p>,</span> <span class=s2>&#34;accuracy&#34;</span><span class=p>:</span> <span class=n>training_accuracy</span><span class=p>})</span>
</span></span></code></pre></div></li></ol><p>Now you can visualize your training progress in the W&amp;B dashboard.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Fine-tuning a closed-source model like ChatGPT can enhance its ability to cater to specific queries, making it more valuable for your use case. By following the steps outlined in this blog post—from preparing your dataset in JSONL format to uploading it and monitoring with W&amp;B—you can leverage the full potential of ChatGPT for your needs.</p><p>As you embark on your fine-tuning journey, remember that iteration is key. Experiment with your dataset and monitor performance continuously to make informed adjustments and improvements. Good luck with your fine-tuning endeavors!</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/first/>First</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/post7/><span class=title>« Prev</span><br><span>Customizing Large Language Models</span>
</a><a class=next href=http://localhost:1313/posts/post9/><span class=title>Next »</span><br><span>Fine-Tuning ChatGPT</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning ChatGPT on x" href="https://x.com/intent/tweet/?text=Fine-Tuning%20ChatGPT&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpost9-copy%2f&amp;hashtags=first"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning ChatGPT on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpost9-copy%2f&amp;title=Fine-Tuning%20ChatGPT&amp;summary=Fine-Tuning%20ChatGPT&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fpost9-copy%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning ChatGPT on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpost9-copy%2f&title=Fine-Tuning%20ChatGPT"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning ChatGPT on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fpost9-copy%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning ChatGPT on whatsapp" href="https://api.whatsapp.com/send?text=Fine-Tuning%20ChatGPT%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fpost9-copy%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning ChatGPT on telegram" href="https://telegram.me/share/url?text=Fine-Tuning%20ChatGPT&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpost9-copy%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning ChatGPT on ycombinator" href="https://news.ycombinator.com/submitlink?t=Fine-Tuning%20ChatGPT&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fpost9-copy%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><footer class=footer><div class=footer__inner><div class=copyright>© 2025 Sérgio Soares · Built with care and open source</div></div></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>